{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54280ccb-b63d-46b8-9a77-b10d82f2cfb3",
   "metadata": {},
   "source": [
    "<font size= 20> network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc123c5-de00-4abc-9d89-7d364c04230e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install jupyterlab-codex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5446c3-86d0-4ef5-8dbc-17bbe24adfe5",
   "metadata": {},
   "source": [
    "to change lon from 0-360 to -180+180 for a saved file go on jupyter terminal and do cdo sellonlatbox,-180,180,-90,90 infile.nc outfile.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e7341-469c-4a2b-8915-eb90d8bf6885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "import timeit\n",
    "import matplotlib as mpl\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pylab as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "import matplotlib.cm as cm\n",
    "from scipy.interpolate import CloughTocher2DInterpolator, LinearNDInterpolator, NearestNDInterpolator\n",
    "import glob\n",
    "import matplotlib.colors as colors\n",
    "import intake\n",
    "import dask\n",
    "from scipy.spatial import cKDTree\n",
    "import networkx as nx\n",
    "import psyplot.project as psy\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from pandas.plotting import lag_plot\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, grangercausalitytests\n",
    "import warnings\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "\n",
    "dask.config.set({\"array.slicing.split_large_chunks\": True}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd87cc-6b12-4a69-89cd-ec478396d091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eerie_cat=intake.open_catalog(\"https://raw.githubusercontent.com/eerie-project/intake_catalogues/main/eerie.yaml\")\n",
    "eerie_cat[\"dkrz.disk.model-output.icon-esm-er.eerie-control-1950.atmos.gr025\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd1e6d1-a4e1-4ace-a29d-943ab01bab38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eerie_dkrz=eerie_cat[\"dkrz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12015c-9131-4506-8420-4d311f59bb09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eerie_dkrz_disk=eerie_dkrz[\"disk\"]\n",
    "#for col in eerie_dkrz_disk:\n",
    "#    print(f\"Description of {col}:\")\n",
    "#    print(eerie_dkrz_disk[col].describe()[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976521dc-c079-4c1e-8719-a7e84f009af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat=eerie_dkrz_disk[\"model-output\"][\"icon-esm-er\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa7a2c-4fc3-4e7b-8a99-c052791a90a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "searchdict=dict(\n",
    "    model=\"ICON\",\n",
    "    realm=\"atmos\",\n",
    "    exp=\"eerie-control-1950\",\n",
    "    var=\"geopotential\",\n",
    "    frequency=\"daily\"\n",
    ")\n",
    "subcat=cat\n",
    "for v in searchdict.values():\n",
    "    subcat=subcat.search(v)\n",
    "list(subcat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd219f83-40e8-4d0d-962b-991c3481b390",
   "metadata": {},
   "source": [
    "# EKE vs FRSHFLUX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b131552-8a4e-49fe-8a77-94742d3c5df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#EKE\n",
    "ds=cat[\"eerie-control-1950.ocean.gr025.2d_daily_mean\"].to_dask()\n",
    "#frshflux\n",
    "\n",
    "atm=cat['eerie-control-1950.atmos.gr025.2d_daily_mean'].to_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79bd7f7-65fd-4a83-8d7a-178a5b6750b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds= ds.sel(time=slice('2008-01-01T23:59:00.000000000', '2012-01-01T23:59:00.000000000'))\n",
    "atm=atm.sel(time=slice('2008-01-01T23:59:00.000000000', '2012-01-01T23:59:00.000000000'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435bc15-397a-49af-9670-49797280f84d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0bd0f4-67e1-4aa4-8031-49a1b51a13ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate EKE\n",
    "#make the mean\n",
    "step=20\n",
    "u_mean=np.mean(ds.u[:,0,::step,::step], axis=0) #mean across axis=0, ovvero il tempo\n",
    "v_mean=np.mean(ds.v[:,0,::step,::step], axis=0)\n",
    "\n",
    "#calculate eddy velocities\n",
    "u_eddy=(ds.u[:,0,::step,::step].values)-(u_mean[:,:].values)\n",
    "v_eddy=(ds.v[:,0,::step,::step].values)-(v_mean[:,:].values)\n",
    "\n",
    "#calculate eke\n",
    "eke_s=(((u_eddy)**2)+((v_eddy)**2))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516f5a8-8bed-419f-ade4-6e97f2cd3f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate EKE\n",
    "#make the mean\n",
    "step=20\n",
    "u_mean=np.mean(ds.u[:,0,::step,::step], axis=0) #mean across axis=0, ovvero il tempo\n",
    "v_mean=np.mean(ds.v[:,0,::step,::step], axis=0)\n",
    "\n",
    "#calculate eddy velocities\n",
    "u_eddy=(ds.u[:,0,::step,::step])-(u_mean[:,:])\n",
    "v_eddy=(ds.v[:,0,::step,::step])-(v_mean[:,:])\n",
    "\n",
    "#calculate eke\n",
    "eke_s=(((u_eddy)**2)+((v_eddy)**2))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e779b5e-0edd-45b6-9613-a306ab597f14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step=20\n",
    "pr=atm['pr'][:,::step,::step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a044b61-c71a-450e-b3cf-69aedd20cfb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eke_monthly=eke_s.resample(time=\"m\").mean()\n",
    "pr_monthly=pr.resample(time=\"m\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a44a100-f13d-4008-8a0c-376a1d61eba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pr_monthly=pr_monthly.values\n",
    "eke_monthly=eke_monthly.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eda7ea-b8a9-4d73-8782-7c0162739f70",
   "metadata": {},
   "source": [
    "# SST vs FRSHFLUX or ATMTEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34071fa7-d566-4d9f-a6dd-5433da8a319c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#time=1095 #3 years\n",
    "ds=cat[\"eerie-control-1950.ocean.gr025.5lev_daily_mean\"].to_dask()\n",
    "atm=cat['eerie-control-1950.atmos.gr025.2d_daily_mean'].to_dask()\n",
    "atm2=cat['eerie-control-1950.atmos.gr025.plev19_daily_mean'].to_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70f746b-521f-413c-98e7-3df257784a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SELECT  YEARS\n",
    "ds= ds.sel(time=slice('2013-01-01T23:59:00.000000000', '2016-01-01T23:59:00.000000000'))\n",
    "atm=atm.sel(time=slice('2013-01-01T23:59:00.000000000', '2016-01-01T23:59:00.000000000'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2987da-822d-478a-b968-247f3f49cd0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861addd5-1c3e-4a08-9e90-dbdd6009fe40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0dec90-89f2-45bd-b39e-698fc4d29dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_monthly=ds['to'][:,0,::step,::step].resample(time=\"m\").mean()\n",
    "pr_monthly=atm['pr'][:,::step,::step].resample(time='m').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b445ab2-ead8-4c38-a881-02307b16fe04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_monthly=to_monthly.values\n",
    "pr_monthly=pr_monthly.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa6603c-b37b-4ea8-87f9-94e2c58286b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c7b96-2629-43a5-84e3-82d2a5a62054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to_monthly=ds['to'][:time,0,::step,::step].resample(time=\"m\").mean()\n",
    "tas_monthly=atm['tas'][:time,0,::step,::step].resample(time='m').mean()\n",
    "#pr_monthly=atm['pr'][:time,::step,::step].resample(time='m').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a53c0-954f-452e-b0e7-edda36505efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tas_monthly.values=tas_monthly.values-273.15 #kelvin to celsius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce57835-32cd-4359-a903-1e543506a36e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tas_monthly=tas_monthly.values\n",
    "#to_monthly=to_monthly.values\n",
    "#pr_monthly=pr_monthly.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a95d6-3837-4372-a642-c6e10c4c4713",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SST vs GEOPOT HEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7881cbc3-2b18-4432-a731-8e31bd35caff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds=cat[\"eerie-control-1950.ocean.gr025.2d_daily_mean\"].to_dask()\n",
    "atm2=cat['eerie-control-1950.atmos.gr025.plev19_daily_mean'].to_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6b31f-6361-42f5-a5d8-44226759274f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SELECT  YEARS\n",
    "ds= ds.sel(time=slice('2013-01-01T23:59:00.000000000', '2016-01-01T23:59:00.000000000'))\n",
    "atm2=atm2.sel(time=slice('2013-01-01T23:59:00.000000000', '2016-01-01T23:59:00.000000000'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f794b5f-f345-4e80-9c49-77dbdc12f9fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#geopotential height\n",
    "step=20\n",
    "\n",
    "#find time that correspond to each other because atm2 starts from 2008-01-01\n",
    "gpsm=atm2['gpsm'][:,7,::step,::step] #il 7 sono 300 hPa\n",
    "to=ds['to'][:,0,::step,::step] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a37780-0611-42af-a75f-1034ae41203a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_monthly=ds['to'][:,0,::step,::step].resample(time=\"m\").mean()\n",
    "gpsm_monthly=atm2['gpsm'][:,7,::step,::step].resample(time='m').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9dc4ed-10e4-48e4-9759-a1ce40d3bfcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_monthly=ds['to'][:,0,::step,::step].resample(time=\"W\").mean() #do weekly to look for non-el niño timescale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47fc0e6-6bdd-4a3f-9768-520c81a6a6fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2a413-b865-434f-a713-ec6f2d3d5a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_monthly=to_monthly.values\n",
    "gpsm_monthly=gpsm_monthly.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0554ca17-47a0-4756-a2ca-0a7e9fbbfb83",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SST vs SPECIFIC HUMIDITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7f45d-8afc-4319-b530-ec064ae43b89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds=cat[\"eerie-control-1950.ocean.gr025.2d_daily_mean\"].to_dask()\n",
    "atm2=cat['eerie-control-1950.atmos.gr025.plev19_daily_mean'].to_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753ecdef-eed8-4879-9923-69b50e64a0e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1604ac0-195b-49e8-a4d8-ec3829d2e6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SELECT YEARS\n",
    "ds= ds.sel(time=slice('2014-01-01T23:59:00.000000000', '2016-01-01T23:59:00.000000000'))\n",
    "atm2=atm2.sel(time=slice('2014-01-01T23:59:00.000000000', '2016-01-01T23:59:00.000000000'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc9fc1-a8ad-4c82-a606-c9a5090d0ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da93321-573a-4d16-bd45-b9dff2bde696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "step=20\n",
    "\n",
    "#find time that correspond to each other because atm2 starts from 2008-01-01\n",
    "\n",
    "to=ds['to'][:,0,::step,::step] \n",
    "hus=atm2['hus'][:,7,::step,::step] #il 7 sono 300 hPa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ccb7f9-4353-4d93-9bbe-1772fea04be5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4438e372-f00c-4dc7-806f-da993e84fe4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a5c0c4-1fbe-48df-a58d-ef052afe4074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_monthly=to.resample(time=\"m\").mean()\n",
    "hus_monthly=hus.resample(time='m').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be5431-e54e-47b1-b4e3-941d295790c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_monthly=to_monthly.values\n",
    "hus_monthly=hus_monthly.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15634efe-22b9-4da8-8e76-035a8717f98c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NOAA SST\n",
    "https://psl.noaa.gov/data/gridded/data.cobe.html\n",
    "1.0° latitude x 1.0° longitude global grid (360x180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b42b12-3118-4a3f-9c0c-b6737c62d1d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "ds=xr.open_dataset('noaa_sst_mon_mean_updated.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b58702-8013-46ea-90af-b0d4e4236c42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf231a-d2ae-4e54-a0cb-0a7ce3e45c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds=ds.sel(time=slice('2013-01-01T23:59:00.000000000', '2016-01-01T23:59:00.000000000'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0523a71-e0e5-4dc2-8386-4c1edde3d8ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step=5\n",
    "\n",
    "#find time that correspond to each other because atm2 starts from 2008-01-01\n",
    "\n",
    "to=ds['sst'][:,::step,::step] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d8e3fb-a51d-4dc6-9954-7b8871cb6398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10293f5-d0d7-4b2f-9197-ce7f020d7f07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_monthly=to.values  #they are already monthly means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f14eef-3e2c-406d-a3df-6447b9ade3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c45ec-2839-49e5-af4b-21d894e350e0",
   "metadata": {},
   "source": [
    "# REMOVE SEASONALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f680e0f-c63f-4d22-a688-9e0d88e7f9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SST\n",
    "\n",
    "var_season=to_monthly\n",
    "# Step 1: Plot the original data for one location (assuming one location for simplicity)\n",
    "location_to_plot = (2, 30)  # latitude and longitude to plot (13,1) corresponds to 40N13E, near sardinia\n",
    "plt.plot(var_season[:, location_to_plot[0], location_to_plot[1]], label='Original Data')\n",
    "plt.title(\"Original Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Create an empty array to store deseasonalized data\n",
    "deseason_temp_sst = np.empty_like(var_season)\n",
    "\n",
    "for lat in range(var_season.shape[1]):\n",
    "    for lon in range(var_season.shape[2]):\n",
    "        if np.all(~np.isnan(var_season[:, lat, lon])):\n",
    "            decomposition = seasonal_decompose(var_season[:, lat, lon], model='additive', period=12)\n",
    "            deseason_temp_sst[:, lat, lon] = var_season[:, lat, lon] - decomposition.seasonal\n",
    "        else:\n",
    "            # If there are NaN values, keep them in the deseasonalized array\n",
    "            deseason_temp_sst[:, lat, lon] = var_season[:, lat, lon]\n",
    "\n",
    "# Step 3: Plot the deseasonalized data for one location (assuming one location for simplicity)\n",
    "plt.plot(deseason_temp_sst[:, location_to_plot[0], location_to_plot[1]], label='Deseasonalized Data')\n",
    "plt.title(\"Deseasonalized Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c31774-9eac-422e-94c8-eb492b125912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PREC\n",
    "#REMEMBER TO SUBSTITUTE TAS WITH THE CURRENT VARIABLE IN deseason_temp_tas\n",
    "var_season=pr_monthly\n",
    "# Step 1: Plot the original data for one location (assuming one location for simplicity)\n",
    "location_to_plot = (12, 1)  # latitude and longitude to plot (13,1) corresponds to 40N13E, near sardinia\n",
    "plt.plot(var_season[:, location_to_plot[0], location_to_plot[1]], label='Original Data')\n",
    "plt.title(\"Original Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Create an empty array to store deseasonalized data\n",
    "deseason_temp_pr = np.empty_like(var_season)\n",
    "\n",
    "for lat in range(var_season.shape[1]):\n",
    "    for lon in range(var_season.shape[2]):\n",
    "        if np.all(~np.isnan(var_season[:, lat, lon])):\n",
    "            decomposition = seasonal_decompose(var_season[:, lat, lon], model='additive', period=12)\n",
    "            deseason_temp_pr[:, lat, lon] = var_season[:, lat, lon] - decomposition.seasonal\n",
    "        else:\n",
    "            # If there are NaN values, keep them in the deseasonalized array\n",
    "            deseason_temp_pr[:, lat, lon] = var_season[:, lat, lon]\n",
    "\n",
    "# Step 3: Plot the deseasonalized data for one location (assuming one location for simplicity)\n",
    "plt.plot(deseason_temp_pr[:, location_to_plot[0], location_to_plot[1]], label='Deseasonalized Data')\n",
    "plt.title(\"Deseasonalized Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7ba20-4552-48d2-9f03-881064e22fd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# normalize data\n",
    "Normalizing by subtracting the mean and dividing by the standard deviation helps remove seasonal trends by making the time series data stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23afca6e-845c-4107-b974-6e16a5554e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SST\n",
    "\n",
    "# Step 1: Calculate the long-term mean and standard deviation for each month\n",
    "monthly_means = np.mean(deseason_temp_sst, axis=0)\n",
    "monthly_stddevs = np.std(deseason_temp_sst, axis=0)\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "normalized_data_sst = (deseason_temp_sst - monthly_means) / monthly_stddevs\n",
    "\n",
    "# Now 'normalized_data' contains the normalized values for each data point\n",
    "location_to_plot = (2, 30)  # latitude and longitude to plot (13,1) corresponds to 40N13E, near sardinia\n",
    "\n",
    "# Step 6: Plot the normalized data for one location (assuming one location for simplicity)\n",
    "plt.plot(normalized_data_sst[:, location_to_plot[0], location_to_plot[1]], label='Normalized Data')\n",
    "plt.title(\"Normalized Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28091252-17a1-45c9-85de-7e6dc483cced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PREC\n",
    "\n",
    "# Step 1: Calculate the long-term mean and standard deviation for each month\n",
    "monthly_means = np.mean(deseason_temp_pr, axis=0)\n",
    "monthly_stddevs = np.std(deseason_temp_pr, axis=0)\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "normalized_data_pr = (deseason_temp_pr - monthly_means) / monthly_stddevs\n",
    "\n",
    "# Now 'normalized_data' contains the normalized values for each data point\n",
    "location_to_plot = (12, 1)  # latitude and longitude to plot (13,1) corresponds to 40N13E, near sardinia\n",
    "\n",
    "# Step 6: Plot the normalized data for one location (assuming one location for simplicity)\n",
    "plt.plot(normalized_data_pr[:, location_to_plot[0], location_to_plot[1]], label='Normalized Data')\n",
    "plt.title(\"Normalized Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f93319a-d93f-41e7-9a55-30d4fa96f786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalized_data_sst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd64be3-6518-42e7-98ac-6f3e88e46725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalized_data_tas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777675c-43e9-4c2a-87d1-2f16a1d6a8fd",
   "metadata": {},
   "source": [
    "<font size=20> Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec138fe-f8f5-4174-b495-fef2d74ca6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    lat_diff = abs(lat1 - lat2)\n",
    "    \n",
    "    # Calculate the minimum longitudinal distance considering the cyclic nature\n",
    "    lon_diff = abs((lon1 - lon2 + 180) % 360 - 180)\n",
    "    \n",
    "    return lat_diff, lon_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eda125-a08b-42d3-bdb4-3e59fcb77b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import radians, sin, cos, sqrt, atan2\n",
    "#DISTANCE IN KM\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    # Radius of the Earth in kilometers (mean value)\n",
    "    radius = 6371.0\n",
    "\n",
    "    # Calculate the distance\n",
    "    distance = radius * c\n",
    "\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0d739-24a5-4a98-9113-9d9b1e2b5e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#OLD AND WRONG doesn't account for longitudes being cyclical\n",
    "# Function to calculate distance between two points in degrees\n",
    "#def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "#    lat_diff = abs(lat1 - lat2)\n",
    "#    lon_diff = abs(lon1 - lon2)\n",
    "#    return lat_diff, lon_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376117a2-2097-46c6-9c6f-2c872bf558bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d487dad-3eaa-4f3d-8e56-6b6931157d01",
   "metadata": {},
   "source": [
    "# Plot EKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828aa7c-e5c5-4dfc-86c8-2175f8ef9704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "lon_mesh, lat_mesh = np.meshgrid(ds.lon[::step], ds.lat[::step])\n",
    "\n",
    "\n",
    "\n",
    "# plot using the simple scatter method:\n",
    "plt.figure(figsize=(15,10))\n",
    "# plot using Robinson projection.\n",
    "ax = plt.axes(projection= ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='110m', color='k')\n",
    "\n",
    "# Fill the continents with a color\n",
    "ax.add_feature(cfeature.LAND, facecolor='cornsilk')\n",
    "\n",
    "ax.set_title('Eddy kinetic energy'+'\\n'+'19th of December 2020 T00:00', size='xx-large')\n",
    "ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', linestyle='--')\n",
    "\n",
    "sc=ax.scatter(lon_mesh,lat_mesh,  c=to[0,:,:], s=4 ,cmap='viridis',\n",
    "              transform=ccrs.PlateCarree(), vmax=0.03)#,vmin=np.min(cut_eke[np.nonzero(cut_eke)]),              vmax=0.05)\n",
    "# Add colorbar\n",
    "cbar=plt.colorbar(sc,orientation='horizontal', pad=0.04)\n",
    "cbar.set_label(r'$m^{2} s^{-2}$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f89618-ef7a-416c-9b5a-09230cc96ea8",
   "metadata": {},
   "source": [
    "<font size=20> Network creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73891a2f-665b-401d-98cd-ce80ed27e406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "n_cpu = 48 #48 is optimal\n",
    "#max number of CPUs I can use is the number of latitudes we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a36a3-b3f4-4521-8066-9f9ea918f550",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#world vs world: NO threshold NO anomalies\n",
    "\n",
    "\n",
    "latitudes = ds.lat.values[::step]\n",
    "longitudes = ds.lon.values[::step]\n",
    "\n",
    "eke_downsized = to_monthly\n",
    "frshflux_downsized = tas_monthly\n",
    "\n",
    "# Get the dimensions of the downsized arrays\n",
    "num_time_steps, num_latitudes, num_longitudes = eke_downsized.shape\n",
    "\n",
    "\n",
    "\n",
    "# Set the correlation threshold\n",
    "correlation_threshold = 0.9\n",
    "# Define a function for parallelizing the computation of correlation\n",
    "def correlation_parallel_func(lat_lon):\n",
    "    lat, lon = lat_lon\n",
    "    S = nx.Graph()\n",
    "    if not np.any(np.isnan(eke_downsized[:, lat, lon])):\n",
    "        # Extract time series at the current 'eke' position\n",
    "        time_series_eke = eke_downsized[:, lat, lon]\n",
    "\n",
    "        # Iterate over all positions (lat, lon) for 'frshflux'\n",
    "        for lat_frshflux in range(num_latitudes):\n",
    "            for lon_frshflux in range(num_longitudes):\n",
    "                if not np.any(np.isnan(frshflux_downsized[:, lat_frshflux, lon_frshflux])):\n",
    "                    # Extract time series at the current 'frshflux' position\n",
    "                    time_series_frshflux = frshflux_downsized[:, lat_frshflux, lon_frshflux]\n",
    "\n",
    "                    # Check for NaN values\n",
    "                    if not (np.any(np.isnan(time_series_eke)) or np.any(np.isnan(time_series_frshflux))):\n",
    "                        # Calculate Pearson correlation coefficient\n",
    "                        correlation, _ = pearsonr(time_series_eke, time_series_frshflux)\n",
    "\n",
    "                        # Check if correlation is above the threshold\n",
    "                        if abs(correlation) > correlation_threshold:\n",
    "                            # Add nodes to the graph\n",
    "                            node_eke = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], 'eke')\n",
    "                            node_frshflux = (ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux], 'frshflux')\n",
    "                            S.add_node(node_eke)\n",
    "                            S.add_node(node_frshflux)\n",
    "\n",
    "                            # Add weighted edge with correlation coefficient as weight\n",
    "                            S.add_weighted_edges_from([(node_eke, node_frshflux, correlation)])\n",
    "    return S\n",
    "\n",
    "# Measure the total computation time\n",
    "start_time_total = timeit.default_timer()\n",
    "\n",
    "# Use executor.map to parallelize the computation of correlation\n",
    "with ProcessPoolExecutor(max_workers=n_cpu) as executor:\n",
    "    S_list = list(executor.map(correlation_parallel_func, [(lat, lon) for lat in range(num_latitudes) for lon in range(num_longitudes)]))\n",
    "\n",
    "end_time_total = timeit.default_timer()\n",
    "\n",
    "# Print the total computation time\n",
    "print(f\"Total computation time for the entire script: {end_time_total - start_time_total:.2f} seconds\")\n",
    "\n",
    "# Access individual results\n",
    "final_S_list = [result for result in S_list]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ebed04-fee5-4834-90c4-8b3a2d1572f3",
   "metadata": {},
   "source": [
    "<font size=10> Pearson: world vs world: With distance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2e652f-52ab-4e4d-8505-8f6779e3b029",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "n_cpu = 48 #48 is optimal\n",
    "#max number of CPUs I can use is the number of latitudes we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cd4f6c-f754-4bbf-a0cd-bd2d45f6a8af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eke_downsized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff125d-292a-4e77-a71b-0448d03fbb33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#world vs world: distance threshold  CURRENTTTT\n",
    "\n",
    "\n",
    "latitudes = ds.lat.values[::step]\n",
    "longitudes = ds.lon.values[::step]\n",
    "\n",
    "eke_downsized = normalized_data_sst\n",
    "frshflux_downsized = normalized_data_sst\n",
    "\n",
    "# Get the dimensions of the downsized arrays\n",
    "num_time_steps, num_latitudes, num_longitudes = eke_downsized.shape\n",
    "correlation_threshold=0.9\n",
    "\n",
    "\n",
    "def correlation_parallel_func(lat_lon):\n",
    "    lat, lon = lat_lon\n",
    "    S = nx.Graph()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if not np.any(np.isnan(eke_downsized[:, lat, lon])):\n",
    "        # Extract time series at the current 'eke' position\n",
    "        time_series_eke = eke_downsized[:, lat, lon]\n",
    "\n",
    "        # Iterate over all positions (lat, lon) for 'frshflux'\n",
    "        for lat_frshflux in range(num_latitudes):\n",
    "            for lon_frshflux in range(num_longitudes):\n",
    "                if not np.any(np.isnan(frshflux_downsized[:, lat_frshflux, lon_frshflux])):\n",
    "                    # Extract time series at the current 'frshflux' position\n",
    "                    time_series_frshflux = frshflux_downsized[:, lat_frshflux, lon_frshflux]\n",
    "\n",
    "                    correlation, _ = pearsonr(time_series_eke, time_series_frshflux)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    if abs(correlation) > correlation_threshold:\n",
    "                        distance_km= haversine_distance(\n",
    "                            ds.lat.values[::step][lat], ds.lon.values[::step][lon],\n",
    "                            ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux]\n",
    "                        )\n",
    "                        \n",
    "\n",
    "                        # Check if nodes are at least 2 degrees apart\n",
    "                        if distance_km>=50:\n",
    "                            \n",
    "                            # Add nodes to the graph\n",
    "                            node_eke = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], 'eke')\n",
    "                            node_frshflux = (ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux], 'frshflux')\n",
    "                            S.add_node(node_eke)\n",
    "                            S.add_node(node_frshflux)\n",
    "#\n",
    "                           # Add weighted edge with correlation coefficient as weight\n",
    "                            S.add_weighted_edges_from([(node_eke, node_frshflux, correlation)])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    #    OR (for distance)\n",
    "                    \n",
    "                    \n",
    "                    # Check if correlation is above the threshold and nodes are far enough apart\n",
    "                   # if abs(correlation) > correlation_threshold:\n",
    "                      #  dist_lat, dist_lon = calculate_distance(\n",
    "                       #     ds.lat.values[::step][lat], ds.lon.values[::step][lon],\n",
    "                       #     ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux]\n",
    "                       # )\n",
    "                        \n",
    "                        # Check if nodes are at least x DEGREES apart\n",
    "                       # if dist_lat >= 20 and dist_lon >= 20:\n",
    "                            \n",
    "                       #     # Add nodes to the graph\n",
    "                       #     node_eke = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], 'eke')\n",
    "                       #     node_frshflux = (ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux], 'frshflux')\n",
    "                       #     S.add_node(node_eke)\n",
    "                         #   S.add_node(node_frshflux)\n",
    "#\n",
    "                            # Add weighted edge with correlation coefficient as weight\n",
    "                        #    S.add_weighted_edges_from([(node_eke, node_frshflux, correlation)])\n",
    "                            \n",
    "    return S\n",
    "\n",
    "\n",
    "# Measure the total computation time\n",
    "start_time_total = timeit.default_timer()\n",
    "\n",
    "# Use executor.map to parallelize the computation of correlation\n",
    "with ProcessPoolExecutor(max_workers=n_cpu) as executor:\n",
    "    S_list = list(executor.map(correlation_parallel_func, [(lat, lon) for lat in range(num_latitudes) for lon in range(num_longitudes)]))\n",
    "\n",
    "end_time_total = timeit.default_timer()\n",
    "\n",
    "# Print the total computation time\n",
    "print(f\"Total computation time for the entire script: {end_time_total - start_time_total:.2f} seconds\")\n",
    "\n",
    "# Access individual results\n",
    "final_S_list = [result for result in S_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad6885-9675-4505-80cb-6b9f51b6f134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W=nx.Graph()\n",
    "W = nx.compose_all(final_S_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae8c1b-a67a-439a-ad8b-eab655b6990d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72075f3a-530d-4f8f-9b8f-7ed2b8172c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SAVE\n",
    "# Convert node tuples to strings before saving\n",
    "#W_str_labels = nx.relabel_nodes(W, {node: str(node) for node in W.nodes()})\n",
    "\n",
    "# Save the graph to a GraphML file\n",
    "#nx.write_graphml(W_str_labels, \"sst_anomalies_2001_2021_step10.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e0d486-cba1-4062-911a-0cbf39a72908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d5b1e-17ae-42bf-bb41-6181f5884bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "70+60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ded89-1102-47af-ac7c-2e4b3d982c4b",
   "metadata": {},
   "source": [
    "<font size=10> GRANGER: world vs world: With distance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9149530b-689f-4bdc-87b3-b197c747e243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "granger_causality_test(normalized_data_sst[:,14,8], normalized_data_tas[:,14,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ca0c9-a863-4416-bfff-3b0714daefb6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'normalized_data_sst' and 'normalized_data_tas' are your downsized and normalized data arrays\n",
    "eke_downsized = normalized_data_sst\n",
    "frshflux_downsized = normalized_data_sst\n",
    "\n",
    "# Get the dimensions of the downsized arrays\n",
    "num_time_steps, num_latitudes, num_longitudes = eke_downsized.shape\n",
    "\n",
    "def granger_causality_test(time_series1, time_series2, max_lag=5):\n",
    "    # Perform Granger causality test\n",
    "    data = np.column_stack((time_series1, time_series2))\n",
    "    results = grangercausalitytests(data, max_lag, verbose=False)\n",
    "\n",
    "    # Get p-value from the test\n",
    "    p_value = results[max_lag][0]['ssr_ftest'][1]\n",
    "\n",
    "    return p_value\n",
    "\n",
    "\n",
    "\n",
    "def correlation_parallel_func(lat_lon):\n",
    "    lat, lon = lat_lon\n",
    "    S = nx.Graph()\n",
    "    \n",
    "    if not np.any(np.isnan(eke_downsized[:, lat, lon])):\n",
    "        # Extract time series at the current 'eke' position\n",
    "        time_series_eke = eke_downsized[:, lat, lon]\n",
    "\n",
    "        # Iterate over all positions (lat, lon) for 'frshflux'\n",
    "        for lat_frshflux in range(num_latitudes):\n",
    "            for lon_frshflux in range(num_longitudes):\n",
    "                if not np.any(np.isnan(frshflux_downsized[:, lat_frshflux, lon_frshflux])):\n",
    "                    # Extract time series at the current 'frshflux' position\n",
    "                    time_series_frshflux = frshflux_downsized[:, lat_frshflux, lon_frshflux]\n",
    "\n",
    "                    # Calculate Granger causality\n",
    "                    p_value = granger_causality_test(time_series_eke, time_series_frshflux)\n",
    "                    \n",
    "\n",
    "                                            # Check if p-value is below the threshold and nodes are far enough apart\n",
    "                    if p_value < 0.05:\n",
    "                        distance_km = calculate_distance(\n",
    "                            ds.lat.values[::step][lat], ds.lon.values[::step][lon],\n",
    "                            ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux]\n",
    "                        )\n",
    "\n",
    "                        # Check if nodes are at least x degrees apart\n",
    "                        if distance_km>=30:\n",
    "                            # Add nodes to the graph\n",
    "                            node_eke = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], 'eke')\n",
    "                            node_frshflux = (ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux], 'frshflux')\n",
    "                            S.add_node(node_eke)\n",
    "                            S.add_node(node_frshflux)\n",
    "\n",
    "                            # Add weighted edge with Granger causality p-value as weight\n",
    "                            S.add_weighted_edges_from([(node_eke, node_frshflux, 1/p_value)]) \n",
    "                                \n",
    "                                \n",
    "                        # OR (distance in DEGREES)\n",
    "                        \n",
    "                        \n",
    "                        # Check if p-value is below the threshold and nodes are far enough apart\n",
    "#                        if p_value < 0.05:\n",
    "#                            dist_lat, dist_lon = calculate_distance(\n",
    "#                                ds.lat.values[::step][lat], ds.lon.values[::step][lon],\n",
    "#                                ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux]\n",
    "#                            )\n",
    "#\n",
    "#                            # Check if nodes are at least x degrees apart\n",
    "#                            if dist_lat >=30 and dist_lon >=30:\n",
    "#                                # Add nodes to the graph\n",
    "#                                node_eke = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], 'eke')\n",
    "#                                node_frshflux = (ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux], 'frshflux')\n",
    "#                                S.add_node(node_eke)\n",
    "#                                S.add_node(node_frshflux)\n",
    "#\n",
    "#                                # Add weighted edge with Granger causality p-value as weight\n",
    "#                                S.add_weighted_edges_from([(node_eke, node_frshflux, 1/p_value)])  # Inverse p-value as weight\n",
    "                    \n",
    "                            \n",
    "    return S\n",
    "\n",
    "# Measure the total computation time\n",
    "start_time_total = timeit.default_timer()\n",
    "\n",
    "# Use executor.map to parallelize the computation of correlation\n",
    "with ProcessPoolExecutor(max_workers=n_cpu) as executor:\n",
    "    S_list = list(executor.map(correlation_parallel_func, [(lat, lon) for lat in range(num_latitudes) for lon in range(num_longitudes)]))\n",
    "\n",
    "end_time_total = timeit.default_timer()\n",
    "\n",
    "# Print the total computation time\n",
    "print(f\"Total computation time for the entire script: {end_time_total - start_time_total:.2f} seconds\")\n",
    "\n",
    "# Access individual results\n",
    "final_S_list = [result for result in S_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723d9d7-e4ad-47f9-b80e-2c42c825ee85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W=nx.Graph()\n",
    "W = nx.compose_all(final_S_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41769268-e3f4-412d-a5b1-9ebe53d4f563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f66f4e-75b2-4c6b-bc54-cb8c8728fb9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalized_data_tas[:,13,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e109b0b-a631-4a55-9e93-c6f1c6dc35c3",
   "metadata": {},
   "source": [
    "<font size=10> GRANGER: tropics vs world: With distance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf95b7a-fd04-476f-a946-948513be0290",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'normalized_data_sst' and 'normalized_data_tas' are your downsized and normalized data arrays\n",
    "eke_downsized = normalized_data_sst\n",
    "frshflux_downsized = normalized_data_sst\n",
    "\n",
    "# Define the latitude range for the tropics (adjust as needed)\n",
    "tropic_lat_range = (-23.5, 23.5)\n",
    "\n",
    "# Get the indices of the latitudes within the tropical range\n",
    "tropic_lat_indices = np.where((ds.lat.values >= tropic_lat_range[0]) & (ds.lat.values <= tropic_lat_range[1]))[0]\n",
    "\n",
    "# Get the dimensions of the downsized arrays\n",
    "num_time_steps, num_latitudes, num_longitudes = eke_downsized.shape\n",
    "\n",
    "def granger_causality_test(time_series1, time_series2, max_lag=6):\n",
    "    # Perform Granger causality test\n",
    "    data = np.column_stack((time_series1, time_series2))\n",
    "    results = grangercausalitytests(data, max_lag, verbose=False)\n",
    "\n",
    "    # Get p-value from the test\n",
    "    p_value = results[max_lag][0]['ssr_ftest'][1]\n",
    "\n",
    "    return p_value\n",
    "\n",
    "\n",
    "def correlation_parallel_func(lat_lon):\n",
    "    lat, lon = lat_lon\n",
    "    S = nx.Graph()\n",
    "    \n",
    "    if lat in tropic_lat_indices:  # Check if the latitude is within the tropics\n",
    "        if not np.any(np.isnan(eke_downsized[:, lat, lon])):\n",
    "            # Extract time series at the current 'eke' position\n",
    "            time_series_eke = eke_downsized[:, lat, lon]\n",
    "\n",
    "            # Iterate over all positions (lat, lon) for 'frshflux'\n",
    "            for lat_frshflux in range(num_latitudes):\n",
    "                for lon_frshflux in range(num_longitudes):\n",
    "                    if not np.any(np.isnan(frshflux_downsized[:, lat_frshflux, lon_frshflux])):\n",
    "                        # Extract time series at the current 'frshflux' position\n",
    "                        time_series_frshflux = frshflux_downsized[:, lat_frshflux, lon_frshflux]\n",
    "\n",
    "                        # Calculate Granger causality\n",
    "                        p_value = granger_causality_test(time_series_eke, time_series_frshflux)\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        # Check if p-value is below the threshold and nodes are far enough apart\n",
    "                        if p_value < 0.08:\n",
    "                            distance_km = calculate_distance(\n",
    "                                ds.lat.values[::step][lat], ds.lon.values[::step][lon],\n",
    "                                ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux]\n",
    "                            )\n",
    "\n",
    "                            # Check if nodes are at least x degrees apart\n",
    "                            if distance_km>=30:\n",
    "                                # Add nodes to the graph\n",
    "                                node_eke = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], 'eke')\n",
    "                                node_frshflux = (ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux], 'frshflux')\n",
    "                                S.add_node(node_eke)\n",
    "                                S.add_node(node_frshflux)\n",
    "\n",
    "                                # Add weighted edge with Granger causality p-value as weight\n",
    "                                S.add_weighted_edges_from([(node_eke, node_frshflux, 1/p_value)]) \n",
    "                                \n",
    "                                \n",
    "                        # OR (distance in DEGREES)\n",
    "                        \n",
    "                        \n",
    "                        # Check if p-value is below the threshold and nodes are far enough apart\n",
    "#                        if p_value < 0.08:\n",
    "#                            dist_lat, dist_lon = calculate_distance(\n",
    "#                                ds.lat.values[::step][lat], ds.lon.values[::step][lon],\n",
    "#                                ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux]\n",
    "#                            )\n",
    "#\n",
    "#                            # Check if nodes are at least x degrees apart\n",
    "#                            if dist_lat >=30 and dist_lon >=30:\n",
    "#                                # Add nodes to the graph\n",
    "#                                node_eke = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], 'eke')\n",
    "#                                node_frshflux = (ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux], 'frshflux')\n",
    "#                                S.add_node(node_eke)\n",
    "#                                S.add_node(node_frshflux)\n",
    "#\n",
    "#                                # Add weighted edge with Granger causality p-value as weight\n",
    "#                                S.add_weighted_edges_from([(node_eke, node_frshflux, 1/p_value)])  # Inverse p-value as weight\n",
    "                                \n",
    "    return S\n",
    "\n",
    "# Measure the total computation time\n",
    "start_time_total = timeit.default_timer()\n",
    "\n",
    "# Use executor.map to parallelize the computation of correlation\n",
    "with ProcessPoolExecutor(max_workers=n_cpu) as executor:\n",
    "    S_list = list(executor.map(correlation_parallel_func, [(lat, lon) for lat in range(num_latitudes) for lon in range(num_longitudes)]))\n",
    "\n",
    "end_time_total = timeit.default_timer()\n",
    "\n",
    "# Print the total computation time\n",
    "print(f\"Total computation time for the entire script: {end_time_total - start_time_total:.2f} seconds\")\n",
    "\n",
    "# Access individual results\n",
    "final_S_list = [result for result in S_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a4bc4-147f-414f-80d2-1cd28a37f764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W=nx.Graph()\n",
    "W = nx.compose_all(final_S_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502bfd2-9ef9-477c-be78-4157cc26a514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W.number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23455fcf-5405-4a0d-878b-ed79e6be9832",
   "metadata": {},
   "source": [
    "<font size=10> Pearson: world vs world: anomalies + distance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a27c4d-1a1c-4f9f-973e-65809b132b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "n_cpu = 48 #48 is optimal\n",
    "#max number of CPUs I can use is the number of latitudes we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01bf8d-e4ea-41ce-a1af-90840236b314",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#world vs world: distance threshold\n",
    "#Without close nodes: in this one we take out any possible edge between geographically close nodes\n",
    "\n",
    "latitudes = ds.lat.values[::step]\n",
    "longitudes = ds.lon.values[::step]\n",
    "\n",
    "eke_downsized = normalized_data\n",
    "frshflux_downsized = normalized_data\n",
    "\n",
    "# Get the dimensions of the downsized arrays\n",
    "num_time_steps, num_latitudes, num_longitudes = eke_downsized.shape\n",
    "correlation_threshold=0.7\n",
    "# Calculate the 5th percentile for 'eke' time series\n",
    "threshold_eke_low = np.percentile(eke_downsized[~np.isnan(eke_downsized)], 10)\n",
    "\n",
    "# Calculate the 5th percentile for 'frshflux' time series\n",
    "threshold_frshflux_low = np.percentile(frshflux_downsized[~np.isnan(frshflux_downsized)], 10)\n",
    "\n",
    "# Calculate the 95th percentile for 'eke' time series\n",
    "threshold_eke_high = np.percentile(eke_downsized[~np.isnan(eke_downsized)], 90)\n",
    "\n",
    "# Calculate the 95th percentile for 'frshflux' time series\n",
    "threshold_frshflux_high = np.percentile(frshflux_downsized[~np.isnan(frshflux_downsized)], 90)\n",
    "\n",
    "def correlation_parallel_func(lat_lon):\n",
    "    lat, lon = lat_lon\n",
    "    S = nx.Graph()\n",
    "    \n",
    "\n",
    "    \n",
    "    if not np.any(np.isnan(eke_downsized[:, lat, lon])):\n",
    "        # Extract time series at the current 'eke' position\n",
    "        time_series_eke = eke_downsized[:, lat, lon]\n",
    "\n",
    "        # Iterate over all positions (lat, lon) for 'frshflux'\n",
    "        for lat_frshflux in range(num_latitudes):\n",
    "            for lon_frshflux in range(num_longitudes):\n",
    "                if not np.any(np.isnan(frshflux_downsized[:, lat_frshflux, lon_frshflux])):\n",
    "                    # Extract time series at the current 'frshflux' position\n",
    "                    time_series_frshflux = frshflux_downsized[:, lat_frshflux, lon_frshflux]\n",
    "\n",
    "                    # Check if both time series have values above their respective 95th and 0.5th percentiles\n",
    "                    if np.any((time_series_eke > threshold_eke_high) & (time_series_eke < threshold_eke_low)) and \\\n",
    "                       np.any((time_series_frshflux > threshold_frshflux_high) & (time_series_frshflux < threshold_frshflux_low)):\n",
    "                        # Calculate Pearson correlation coefficient\n",
    "                        correlation, _ = pearsonr(time_series_eke, time_series_frshflux)\n",
    "\n",
    "                        if abs(correlation) > correlation_threshold:\n",
    "                            distance_km= haversine_distance(\n",
    "                                ds.lat.values[::step][lat], ds.lon.values[::step][lon],\n",
    "                                ds.lat.values[::step][lat_tas], ds.lon.values[::step][lon_tas]\n",
    "                            )\n",
    "\n",
    "\n",
    "                            # Check if nodes are at least 2 degrees apart\n",
    "                            if distance_km>=50:\n",
    "\n",
    "                                # Add nodes to the graph\n",
    "                                node_eke = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], 'eke')\n",
    "                                node_frshflux = (ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux], 'frshflux')\n",
    "                                S.add_node(node_eke)\n",
    "                                S.add_node(node_frshflux)\n",
    "    #\n",
    "                                 Add weighted edge with correlation coefficient as weight\n",
    "                            #   S.add_weighted_edges_from([(node_eke, node_frshflux, correlation)])\n",
    "\n",
    "\n",
    "\n",
    "                        #    OR (for distance threshold in degrees)\n",
    "\n",
    "\n",
    "                        # Check if correlation is above the threshold and nodes are far enough apart\n",
    "                       # if abs(correlation) > correlation_threshold:\n",
    "                          #  dist_lat, dist_lon = calculate_distance(\n",
    "                           #     ds.lat.values[::step][lat], ds.lon.values[::step][lon],\n",
    "                           #     ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux]\n",
    "                           # )\n",
    "\n",
    "                            # Check if nodes are at least x DEGREES apart\n",
    "                           # if dist_lat >= 20 and dist_lon >= 20:\n",
    "\n",
    "                           #     # Add nodes to the graph\n",
    "                           #     node_eke = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], 'eke')\n",
    "                           #     node_frshflux = (ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux], 'frshflux')\n",
    "                           #     S.add_node(node_eke)\n",
    "                             #   S.add_node(node_frshflux)\n",
    "    #\n",
    "                                # Add weighted edge with correlation coefficient as weight\n",
    "                            #    S.add_weighted_edges_from([(node_eke, node_frshflux, correlation)])\n",
    " \n",
    "    return S\n",
    "\n",
    "\n",
    "# Measure the total computation time\n",
    "start_time_total = timeit.default_timer()\n",
    "\n",
    "# Use executor.map to parallelize the computation of correlation\n",
    "with ProcessPoolExecutor(max_workers=n_cpu) as executor:\n",
    "    S_list = list(executor.map(correlation_parallel_func, [(lat, lon) for lat in range(num_latitudes) for lon in range(num_longitudes)]))\n",
    "\n",
    "end_time_total = timeit.default_timer()\n",
    "\n",
    "# Print the total computation time\n",
    "print(f\"Total computation time for the entire script: {end_time_total - start_time_total:.2f} seconds\")\n",
    "\n",
    "# Access individual results\n",
    "final_S_list = [result for result in S_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d185e4d7-a145-4210-ab5a-97c3000f6279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W=nx.Graph()\n",
    "W = nx.compose_all(final_S_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cebfd5-1b89-4d9f-b3e2-a94adb78c681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6adf5-fec7-4f82-a836-1b8a4f34e265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "458adea7-f4d1-4e0f-a5f8-dc8d3fa85971",
   "metadata": {},
   "source": [
    "<font size=8> Pearson: world vs world (only SST): anomalies + distance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f0d3e-0173-4e8a-bd02-42c963d6724a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "latitudes = ds.lat.values[::step]\n",
    "longitudes = ds.lon.values[::step]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to calculate the 95th percentile for each grid point\n",
    "def calculate_percentile(data, percentile=95):\n",
    "    return np.percentile(data, percentile, axis=0)\n",
    "\n",
    "# Get the 95th percentile for each grid point in tas_monthly\n",
    "tas_percentile_95 = calculate_percentile(to_monthly)\n",
    "correlation_threshold=-0.9\n",
    "# Define a function for parallelizing the computation of correlation\n",
    "def correlation_parallel_func(lat_lon):\n",
    "    lat, lon = lat_lon\n",
    "    S = nx.Graph()\n",
    "\n",
    "    # Extract time series at the current position\n",
    "    time_series = to_monthly[:, lat, lon]\n",
    "\n",
    "    # Check for NaN values\n",
    "    if not np.any(np.isnan(time_series)):\n",
    "        # Get the 95th percentile for the current grid point in to_monthly\n",
    "        threshold_to_monthly = np.percentile(time_series, 95)\n",
    "\n",
    "        # Iterate over all positions (lat, lon) in the dataset\n",
    "        for lat_tas in range(num_latitudes):\n",
    "            for lon_tas in range(num_longitudes):\n",
    "                if (lat, lon) != (lat_tas, lon_tas):  # Skip the same position\n",
    "                    # Check if nodes are at least 2 degrees apart\n",
    "                    distance_km= harvestine_distance(\n",
    "                        ds.lat.values[::step][lat], ds.lon.values[::step][lon],\n",
    "                        ds.lat.values[::step][lat_tas], ds.lon.values[::step][lon_tas]\n",
    "                    )\n",
    "\n",
    "                    if distance_km >= 50:\n",
    "                        # Extract time series at the current position\n",
    "                        time_series_tas = to_monthly[:, lat_tas, lon_tas]\n",
    "\n",
    "                        # Check for NaN values\n",
    "                        if not np.any(np.isnan(time_series_tas)):\n",
    "                            # Check if temperatures in to_monthly are above the 95th percentile\n",
    "                            if np.all(time_series_tas > threshold_to_monthly):\n",
    "                                # Calculate Pearson correlation coefficient\n",
    "                                correlation, _ = pearsonr(time_series, time_series_tas)\n",
    "\n",
    "                                # Check if correlation is above the threshold\n",
    "                                if correlation < correlation_threshold:\n",
    "                                    # Add nodes to the graph\n",
    "                                    node_current = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], 'eke')\n",
    "                                    node_other = (ds.lat.values[::step][lat_tas], ds.lon.values[::step][lon_tas], 'frshflux')\n",
    "                                    S.add_node(node_current)\n",
    "                                    S.add_node(node_other)\n",
    "\n",
    "                                    # Add weighted edge with correlation coefficient as weight\n",
    "                                    S.add_weighted_edges_from([(node_current, node_other, correlation)])\n",
    "                \n",
    "                \n",
    "                \n",
    "                #   OR   (distance in DEGREES)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "    #            if (lat, lon) != (lat_tas, lon_tas):  # Skip the same position\n",
    "    #                # Check if nodes are at least 2 degrees apart\n",
    "    #                dist_lat, dist_lon = calculate_distance(\n",
    "    #                    ds.lat.values[::step][lat], ds.lon.values[::step][lon],\n",
    "    #                    ds.lat.values[::step][lat_tas], ds.lon.values[::step][lon_tas]\n",
    "    #                )\n",
    "#\n",
    "    #                if dist_lat >= 50 and dist_lon >= 50:\n",
    "    #                    # Extract time series at the current position\n",
    "  #                      time_series_tas = to_monthly[:, lat_tas, lon_tas]\n",
    "#\n",
    "#                        # Check for NaN values\n",
    "#                        if not np.any(np.isnan(time_series_tas)):\n",
    "#                            # Check if temperatures in to_monthly are above the 95th percentile\n",
    "#                            if np.all(time_series_tas > threshold_to_monthly):\n",
    "#                                # Calculate Pearson correlation coefficient\n",
    "#                                correlation, _ = pearsonr(time_series, time_series_tas)\n",
    "#\n",
    "#                                # Check if correlation is above the threshold\n",
    "#                                if correlation < correlation_threshold:\n",
    "#                                    # Add nodes to the graph\n",
    "#                                    node_current = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], 'eke')\n",
    "#                                    node_other = (ds.lat.values[::step][lat_tas], ds.lon.values[::step][lon_tas], 'frshflux')\n",
    "#                                    S.add_node(node_current)\n",
    "#                                    S.add_node(node_other)\n",
    "#\n",
    "#                                    # Add weighted edge with correlation coefficient as weight\n",
    "#                                    S.add_weighted_edges_from([(node_current, node_other, correlation)])\n",
    "    return S\n",
    "\n",
    "\n",
    "# Measure the total computation time\n",
    "start_time_total = timeit.default_timer()\n",
    "\n",
    "# Use executor.map to parallelize the computation of correlation\n",
    "with ProcessPoolExecutor(max_workers=n_cpu) as executor:\n",
    "    S_list = list(executor.map(correlation_parallel_func, [(lat, lon) for lat in range(num_latitudes) for lon in range(num_longitudes)]))\n",
    "\n",
    "end_time_total = timeit.default_timer()\n",
    "\n",
    "# Print the total computation time\n",
    "print(f\"Total computation time for the entire script: {end_time_total - start_time_total:.2f} seconds\")\n",
    "\n",
    "# Access individual results\n",
    "final_S_list = [result for result in S_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3507f1f2-c12f-4a8d-9210-d66ef281a464",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "W=nx.Graph()\n",
    "W = nx.compose_all(final_S_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3759619-9bf7-4d51-a184-c7a1f12bda48",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "W.number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d4bf2e-ec60-481e-a825-5007d9b72a55",
   "metadata": {},
   "source": [
    "<font size=10> PEARSON: tropics vs world: With distance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441e065-36d2-477b-a970-ed20dfc2d2a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DEGREES DISTANCE\n",
    "\n",
    "latitudes = ds.lat.values[::step]\n",
    "longitudes = ds.lon.values[::step]\n",
    "\n",
    "eke_downsized = normalized_data_sst\n",
    "frshflux_downsized = normalized_data_sst\n",
    "\n",
    "# Get the dimensions of the downsized arrays\n",
    "num_time_steps, num_latitudes, num_longitudes = eke_downsized.shape\n",
    "correlation_threshold=0.85\n",
    "# Define the latitude bounds for the tropics (e.g., between -23.5 and 23.5 degrees)\n",
    "tropics_lat_bounds = (-23.5, 23.5)\n",
    "\n",
    "# Function to check if a latitude is within the tropics\n",
    "def is_in_tropics(lat):\n",
    "    return tropics_lat_bounds[0] <= lat <= tropics_lat_bounds[1]\n",
    "\n",
    "\n",
    "\n",
    "# Define a function for parallelizing the computation of correlation\n",
    "def correlation_parallel_func(lat_lon):\n",
    "    lat, lon = lat_lon\n",
    "    S = nx.Graph()\n",
    "\n",
    "    # Check if the latitude is within the tropics for to_monthly\n",
    "    if is_in_tropics(ds.lat.values[::step][lat]):\n",
    "        # Extract time series at the current 'to_monthly' position\n",
    "        time_series_to_monthly = eke_downsized[:, lat, lon]\n",
    "\n",
    "        # Iterate over all positions (lat, lon) for 'tas_monthly'\n",
    "        for lat_tas in range(num_latitudes):\n",
    "            for lon_tas in range(num_longitudes):\n",
    "                # Extract time series at the current 'tas_monthly' position\n",
    "                time_series_tas = frshflux_downsized[:, lat_tas, lon_tas]\n",
    "\n",
    "                # Check for NaN values\n",
    "                if not (np.any(np.isnan(time_series_to_monthly)) or np.any(np.isnan(time_series_tas))):\n",
    "                    # Calculate Pearson correlation coefficient\n",
    "                    correlation, _ = pearsonr(time_series_to_monthly, time_series_tas)\n",
    "\n",
    "                    # Check if correlation is above the threshold and nodes are far enough apart\n",
    "                    if abs(correlation) > correlation_threshold:\n",
    "                        dist_lat, dist_lon = calculate_distance(\n",
    "                            ds.lat.values[::step][lat], ds.lon.values[::step][lon],\n",
    "                            ds.lat.values[::step][lat_tas], ds.lon.values[::step][lon_tas]\n",
    "                        )\n",
    "\n",
    "                        # Check if nodes are at least 2 degrees apart\n",
    "                        if dist_lat >= 20 and dist_lon >= 20:\n",
    "                            \n",
    "                            # Add nodes to the graph\n",
    "                            node_to_monthly = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], 'eke')\n",
    "                            node_tas_monthly = (ds.lat.values[::step][lat_tas], ds.lon.values[::step][lon_tas], 'frshflux')\n",
    "                            S.add_node(node_to_monthly)\n",
    "                            S.add_node(node_tas_monthly)\n",
    "\n",
    "                            # Add weighted edge with correlation coefficient as weight\n",
    "                            S.add_weighted_edges_from([(node_to_monthly, node_tas_monthly, correlation)])\n",
    "    return S\n",
    "\n",
    "# Measure the total computation time\n",
    "start_time_total = timeit.default_timer()\n",
    "\n",
    "# Use executor.map to parallelize the computation of correlation\n",
    "with ProcessPoolExecutor(max_workers=n_cpu) as executor:\n",
    "    S_list = list(executor.map(correlation_parallel_func, [(lat, lon) for lat in range(num_latitudes) for lon in range(num_longitudes)]))\n",
    "\n",
    "end_time_total = timeit.default_timer()\n",
    "\n",
    "# Print the total computation time\n",
    "print(f\"Total computation time for the entire script: {end_time_total - start_time_total:.2f} seconds\")\n",
    "\n",
    "# Access individual results\n",
    "final_S_list = [result for result in S_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25254b9d-6502-480b-902c-85ea1c91d3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd45ac67-f9eb-4ef8-9926-949c27f42289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1b9934-4800-4518-8e45-ead5fda6fff2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#HARVESTINE DISTANCE (KM)\n",
    "\n",
    "\n",
    "latitudes = ds.lat.values[::step]\n",
    "longitudes = ds.lon.values[::step]\n",
    "\n",
    "eke_downsized = normalized_data_sst\n",
    "frshflux_downsized = normalized_data_sst\n",
    "\n",
    "# Get the dimensions of the downsized arrays\n",
    "num_time_steps, num_latitudes, num_longitudes = eke_downsized.shape\n",
    "correlation_threshold=0.85\n",
    "# Define the latitude bounds for the tropics (e.g., between -23.5 and 23.5 degrees)\n",
    "tropics_lat_bounds = (-23.5, 23.5)\n",
    "\n",
    "# Function to check if a latitude is within the tropics\n",
    "def is_in_tropics(lat):\n",
    "    return tropics_lat_bounds[0] <= lat <= tropics_lat_bounds[1]\n",
    "\n",
    "\n",
    "\n",
    "# Define a function for parallelizing the computation of correlation\n",
    "def correlation_parallel_func(lat_lon):\n",
    "    lat, lon = lat_lon\n",
    "    S = nx.Graph()\n",
    "\n",
    "    # Check if the latitude is within the tropics for to_monthly\n",
    "    if is_in_tropics(ds.lat.values[::step][lat]):\n",
    "        print(lat)\n",
    "        # Extract time series at the current 'to_monthly' position\n",
    "        time_series_to_monthly = eke_downsized[:, lat, lon]\n",
    "\n",
    "        # Iterate over all positions (lat, lon) for 'tas_monthly'\n",
    "        for lat_tas in range(num_latitudes):\n",
    "            for lon_tas in range(num_longitudes):\n",
    "                # Extract time series at the current 'tas_monthly' position\n",
    "                time_series_tas = frshflux_downsized[:, lat_tas, lon_tas]\n",
    "\n",
    "                # Check for NaN values\n",
    "                if not (np.any(np.isnan(time_series_to_monthly)) or np.any(np.isnan(time_series_tas))):\n",
    "                    # Calculate Pearson correlation coefficient\n",
    "                    correlation, _ = pearsonr(time_series_to_monthly, time_series_tas)\n",
    "\n",
    "                    # Check if correlation is above the threshold and nodes are far enough apart\n",
    "                    if abs(correlation) > correlation_threshold:\n",
    "                        distance_km= haversine_distance(\n",
    "                            ds.lat.values[::step][lat], ds.lon.values[::step][lon],\n",
    "                            ds.lat.values[::step][lat_tas], ds.lon.values[::step][lon_tas]\n",
    "                        )\n",
    "                        \n",
    "\n",
    "                        # Check if nodes are at least 2 degrees apart\n",
    "                        if distance_km>=50:\n",
    "                            \n",
    "                            # Add nodes to the graph\n",
    "                            node_to_monthly = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], 'eke')\n",
    "                            node_tas_monthly = (ds.lat.values[::step][lat_tas], ds.lon.values[::step][lon_tas], 'frshflux')\n",
    "                            S.add_node(node_to_monthly)\n",
    "                            S.add_node(node_tas_monthly)\n",
    "\n",
    "                            # Add weighted edge with correlation coefficient as weight\n",
    "                            S.add_weighted_edges_from([(node_to_monthly, node_tas_monthly, correlation)])\n",
    "    return S\n",
    "\n",
    "# Measure the total computation time\n",
    "start_time_total = timeit.default_timer()\n",
    "\n",
    "# Use executor.map to parallelize the computation of correlation\n",
    "with ProcessPoolExecutor(max_workers=n_cpu) as executor:\n",
    "    S_list = list(executor.map(correlation_parallel_func, [(lat, lon) for lat in range(num_latitudes) for lon in range(num_longitudes)]))\n",
    "\n",
    "end_time_total = timeit.default_timer()\n",
    "\n",
    "# Print the total computation time\n",
    "print(f\"Total computation time for the entire script: {end_time_total - start_time_total:.2f} seconds\")\n",
    "\n",
    "# Access individual results\n",
    "final_S_list = [result for result in S_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb32bf-cdb7-48d7-9ae9-e7ceb33328bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W=nx.Graph()\n",
    "W = nx.compose_all(final_S_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68abca67-c91f-4d7c-8d39-f8c930f36d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76496e1b-03e3-4c92-8021-a2bdd22f1f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ecea5-3505-4bd3-a2aa-cee408c063b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9720b846-8945-4353-a467-906a24bcfdc6",
   "metadata": {},
   "source": [
    "<font size=10> PEARSON: Eastern pacific vs world: With distance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50431d91-aaea-45f0-af52-c168c428fa8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the latitude and longitude bounds for the Eastern Pacific region\n",
    "eastern_pacific_lat_bounds = (-20, 10)\n",
    "eastern_pacific_lon_bounds = (-180, -79.5)\n",
    "\n",
    "# Extract data for the Eastern Pacific region\n",
    "eastern_pacific_region = ds.sel(\n",
    "    lat=slice(eastern_pacific_lat_bounds[0], eastern_pacific_lat_bounds[1]),\n",
    "    lon=slice(eastern_pacific_lon_bounds[0], eastern_pacific_lon_bounds[1])\n",
    ")\n",
    "\n",
    "# Plot the Eastern Pacific region\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.set_global()\n",
    "ax.coastlines()\n",
    "ax.gridlines(draw_labels=True)\n",
    "\n",
    "# Plot the selected region\n",
    "ax.add_patch(plt.Rectangle(\n",
    "    (eastern_pacific_lon_bounds[0], eastern_pacific_lat_bounds[0]),\n",
    "    eastern_pacific_lon_bounds[1] - eastern_pacific_lon_bounds[0],\n",
    "    eastern_pacific_lat_bounds[1] - eastern_pacific_lat_bounds[0],\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    facecolor='none',\n",
    "    edgecolor='red',\n",
    "    linewidth=2,\n",
    "    label='Eastern Pacific Region'\n",
    "))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac9d97f-40ce-448f-88c8-20257bf1391a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#nota bene: per il file noaa caricato sul pc\n",
    "#the longitudes are already converted to -180,180. this code has to be adjusted if the file's longitudes are 0-360\n",
    "\n",
    "latitudes = ds.lat.values[::step]\n",
    "longitudes = ds.lon.values[::step]\n",
    "\n",
    "eke_downsized = normalized_data_sst\n",
    "frshflux_downsized = normalized_data_sst\n",
    "\n",
    "# Get the dimensions of the downsized arrays\n",
    "num_time_steps, num_latitudes, num_longitudes = eke_downsized.shape\n",
    "correlation_threshold = 0.85\n",
    "\n",
    "# Define the latitude and longitude bounds for the Eastern Pacific region\n",
    "eastern_pacific_lat_bounds = (-20, 10)\n",
    "eastern_pacific_lon_bounds = (-180, -79.5)  # Adjust the longitude bounds as needed\n",
    "\n",
    "# Function to check if a latitude and longitude are within the Eastern Pacific region\n",
    "def is_in_eastern_pacific(lat, lon):\n",
    "    return (eastern_pacific_lat_bounds[0] <= lat <= eastern_pacific_lat_bounds[1] and\n",
    "            eastern_pacific_lon_bounds[0] <= lon <= eastern_pacific_lon_bounds[1])\n",
    "\n",
    "# Define a function for parallelizing the computation of correlation\n",
    "def correlation_parallel_func(lat_lon):\n",
    "    lat, lon = lat_lon\n",
    "    S = nx.Graph()\n",
    "\n",
    "    # Check if the latitude and longitude are within the Eastern Pacific region\n",
    "    if is_in_eastern_pacific(ds.lat.values[::step][lat], ds.lon.values[::step][lon]):\n",
    "        # Extract time series at the current position in the Eastern Pacific\n",
    "        time_series_eastern_pacific = eke_downsized[:, lat, lon]\n",
    "\n",
    "        # Iterate over all positions (lat, lon) for the rest of the world\n",
    "        for lat_rest_of_world in range(num_latitudes):\n",
    "            for lon_rest_of_world in range(num_longitudes):\n",
    "                # Skip positions within the Eastern Pacific\n",
    "                if is_in_eastern_pacific(ds.lat.values[::step][lat_rest_of_world], ds.lon.values[::step][lon_rest_of_world]):\n",
    "                    continue\n",
    "\n",
    "                # Extract time series at the current position in the rest of the world\n",
    "                time_series_rest_of_world = frshflux_downsized[:, lat_rest_of_world, lon_rest_of_world]\n",
    "\n",
    "                # Check for NaN values\n",
    "                if not (np.any(np.isnan(time_series_eastern_pacific)) or np.any(np.isnan(time_series_rest_of_world))):\n",
    "\n",
    "                    # Calculate Pearson correlation coefficient\n",
    "                    correlation = np.nan  # Default to NaN\n",
    "\n",
    "                    if not np.all(time_series_eastern_pacific == time_series_eastern_pacific[0]) and not np.all(time_series_rest_of_world == time_series_rest_of_world[0]):\n",
    "                        correlation, _ = pearsonr(time_series_eastern_pacific, time_series_rest_of_world)\n",
    "\n",
    "                    # Check if correlation is above the threshold and nodes are far enough apart\n",
    "                    if abs(correlation) > correlation_threshold:\n",
    "                        distance_km = haversine_distance(\n",
    "                            ds.lat.values[::step][lat], ds.lon.values[::step][lon],\n",
    "                            ds.lat.values[::step][lat_rest_of_world], ds.lon.values[::step][lon_rest_of_world]\n",
    "                        )\n",
    "\n",
    "                        # Check if nodes are at least 20 km apart\n",
    "                        if distance_km >= 2000:\n",
    "                            # Add nodes to the graph with proper categorization\n",
    "                            node_category_eastern_pacific = 'eastern_pacific'\n",
    "                            node_eastern_pacific = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], node_category_eastern_pacific)\n",
    "\n",
    "                            node_category_rest_of_world = 'rest_of_the_world'\n",
    "                            node_rest_of_world = (ds.lat.values[::step][lat_rest_of_world], ds.lon.values[::step][lon_rest_of_world], node_category_rest_of_world)\n",
    "\n",
    "                            S.add_node(node_eastern_pacific)\n",
    "                            S.add_node(node_rest_of_world)\n",
    "\n",
    "                            # Add weighted edge with correlation coefficient as weight\n",
    "                            S.add_weighted_edges_from([(node_eastern_pacific, node_rest_of_world, correlation)])\n",
    "\n",
    "\n",
    "    return S\n",
    "\n",
    "# Measure the total computation time\n",
    "start_time_total = timeit.default_timer()\n",
    "\n",
    "# Use executor.map to parallelize the computation of correlation\n",
    "with ProcessPoolExecutor(max_workers=n_cpu) as executor:\n",
    "    S_list = list(executor.map(correlation_parallel_func, [(lat, lon) for lat in range(num_latitudes) for lon in range(num_longitudes)]))\n",
    "\n",
    "end_time_total = timeit.default_timer()\n",
    "\n",
    "# Print the total computation time\n",
    "print(f\"Total computation time for the entire script: {end_time_total - start_time_total:.2f} seconds\")\n",
    "\n",
    "# Access individual results\n",
    "final_S_list = [result for result in S_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8addd9ef-a280-4e6c-b1d1-f73129d32168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W=nx.Graph()\n",
    "W = nx.compose_all(final_S_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b21d47-ebb9-493b-bc88-fa7222b5cef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W.number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b25ae1-d108-460d-b68f-a3b875559d61",
   "metadata": {},
   "source": [
    "<font size=10> Pearson:tropics vs world: anomalies + distance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57aad35-20ec-4cae-808e-f0d91448d08e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tropics vs world: distance and anomalies\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "latitudes = ds.lat.values[::step]\n",
    "longitudes = ds.lon.values[::step]\n",
    "\n",
    "eke_downsized = normalized_data\n",
    "frshflux_downsized = normalized_data\n",
    "\n",
    "# Get the dimensions of the downsized arrays\n",
    "num_time_steps, num_latitudes, num_longitudes = eke_downsized.shape\n",
    "\n",
    "correlation_threshold = 0.9\n",
    "\n",
    "# Define the latitude bounds for the tropics (e.g., between -23.5 and 23.5 degrees)\n",
    "tropics_lat_bounds = (-23.5, 23.5)\n",
    "\n",
    "# Function to check if a latitude is within the tropics\n",
    "def is_in_tropics(lat):\n",
    "    return tropics_lat_bounds[0] <= lat <= tropics_lat_bounds[1]\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to calculate the 95th percentile for each grid point\n",
    "def calculate_percentile(data, percentile=9):\n",
    "    return np.percentile(data, percentile, axis=0)\n",
    "\n",
    "# ...\n",
    "\n",
    "# Get the 95th percentile for each grid point in tas_monthly\n",
    "tas_percentile_95 = calculate_percentile(to_monthly)\n",
    "\n",
    "def correlation_parallel_func(lat_lon):\n",
    "    lat, lon = lat_lon\n",
    "    S = nx.Graph()\n",
    "\n",
    "    # Check if the latitude is within the tropics for to_monthly\n",
    "    if is_in_tropics(ds.lat.values[::step][lat]):\n",
    "    \n",
    "        # Extract time series at the current 'to_monthly' position\n",
    "        time_series_to_monthly = to_monthly[:, lat, lon]\n",
    "\n",
    "        # Get the 95th percentile threshold for the current grid point in tas_monthly\n",
    "        threshold_tas = tas_percentile_95[lat, lon]\n",
    "\n",
    "        # Iterate over all positions (lat, lon) for 'tas_monthly'\n",
    "        for lat_tas in range(num_latitudes):\n",
    "            for lon_tas in range(num_longitudes):\n",
    "                # Check if the latitude is within the tropics for tas_monthly\n",
    "                if not is_in_tropics(ds.lat.values[::step][lat_tas]):\n",
    "                    # Extract time series at the current 'tas_monthly' position\n",
    "                    time_series_tas = frshflux_downsized[:, lat_tas, lon_tas]\n",
    "\n",
    "                    # Check for NaN values\n",
    "                    if not (np.any(np.isnan(time_series_to_monthly)) or np.any(np.isnan(time_series_tas))):\n",
    "                        # Check if temperatures in tas_monthly are above the 95th percentile\n",
    "                        if np.all(time_series_tas > threshold_tas):\n",
    "                            # Calculate Pearson correlation coefficient\n",
    "                            correlation, _ = pearsonr(time_series_to_monthly, time_series_tas)\n",
    "\n",
    "                            # Check if correlation is above the threshold and nodes are at least 2 degrees apart\n",
    "                            if abs(correlation) > correlation_threshold:\n",
    "                                distance_km= harvestine_distance(\n",
    "                                    ds.lat.values[::step][lat], ds.lon.values[::step][lon],\n",
    "                                    ds.lat.values[::step][lat_tas], ds.lon.values[::step][lon_tas]\n",
    "                                )\n",
    "\n",
    "                                # Check if nodes are at least 2 degrees apart\n",
    "                                if distance_km>= 30:\n",
    "                                    # Add nodes to the graph\n",
    "                                    node_to_monthly = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], 'eke')\n",
    "                                    node_tas_monthly = (ds.lat.values[::step][lat_tas], ds.lon.values[::step][lon_tas], 'frshflux')\n",
    "                                    S.add_node(node_to_monthly)\n",
    "                                    S.add_node(node_tas_monthly)\n",
    "\n",
    "                                    # Add weighted edge with correlation coefficient as weight\n",
    "                                    S.add_weighted_edges_from([(node_to_monthly, node_tas_monthly, correlation)])\n",
    "    return S\n",
    "\n",
    "\n",
    "# Measure the total computation time\n",
    "start_time_total = timeit.default_timer()\n",
    "\n",
    "# Use executor.map to parallelize the computation of correlation\n",
    "with ProcessPoolExecutor(max_workers=n_cpu) as executor:\n",
    "    S_list = list(executor.map(correlation_parallel_func, [(lat, lon) for lat in range(num_latitudes) for lon in range(num_longitudes)]))\n",
    "\n",
    "end_time_total = timeit.default_timer()\n",
    "\n",
    "# Print the total computation time\n",
    "print(f\"Total computation time for the entire script: {end_time_total - start_time_total:.2f} seconds\")\n",
    "\n",
    "# Access individual results\n",
    "final_S_list = [result for result in S_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4691b0-5845-41c0-805e-cbc75c3ff90d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "W=nx.Graph()\n",
    "for i in range(len(final_S_list)):\n",
    "    W=nx.compose(W,final_S_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa3cc57-0568-4eee-9de0-3696a1344c43",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "W.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910cd42-8aba-4fe5-af9c-2fe832e69d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ad1cc-0063-452c-b5ea-04b52159a0d7",
   "metadata": {},
   "source": [
    "<font size=30> PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154cf6e6-11e6-45f0-b184-e61bde412705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# Generate layout for visualization with Cartopy (changes to long,lat)\n",
    "pos = {node: (np.mod(node[1], 360), node[0]) for node in W.nodes()} # No need to modify longitudes, assuming they are already in the range -180 to +180\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12), subplot_kw={'projection': ccrs.InterruptedGoodeHomolosine()})\n",
    "# ax.set_global()\n",
    "# ax.set_extent([-180, 180, -90, 90], ccrs.PlateCarree())\n",
    "ax.add_feature(cfeature.LAND, zorder=1, edgecolor='k')\n",
    "#ax.set_extent([-180, 180, -90, 90], crs=ccrs.InterruptedGoodeHomolosine())\n",
    "# Add world map and gridlines\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
    "ax.gridlines(linewidth=0.5, linestyle='dashed', alpha=0.5, draw_labels=True)\n",
    "\n",
    "# Extract edge weights for edge width\n",
    "edge_weights = [data['weight'] for _, _, data in W.edges(data=True)]\n",
    "\n",
    "# Separate nodes based on type\n",
    "eke_nodes = [node for node in W.nodes() if node[2] == 'eastern_pacific']\n",
    "frshflux_nodes = [node for node in W.nodes() if node[2] == 'rest_of_the_world']\n",
    "\n",
    "#PARTE VECCHIA======================================================================\n",
    "#for edge, weight in zip(G.edges(), edge_weights):\n",
    "#    normalized_weight = (weight - min(edge_weights)) / (max(edge_weights) - min(edge_weights))  # Normalize weights between 0 and 1\n",
    "#    edge_width = 1 + 4 * normalized_weight  # Adjust the multiplier for the desired range of edge widths\n",
    "#    ax.plot([pos[edge[0]][0], pos[edge[1]][0]],\n",
    "#            [pos[edge[0]][1], pos[edge[1]][1]], \n",
    "#            alpha=0.1, color=\"m\", linewidth=edge_width)\n",
    "\n",
    "# Plot nodes\n",
    "#ax.scatter([pos[node][0] for node in eke_nodes], [np.mod(pos[node][1],180) for node in eke_nodes], s=10, color=\"red\", alpha=0.9, label='eke',zorder=20001)\n",
    "#ax.scatter([pos[node][0] for node in frshflux_nodes], [pos[node][1] for node in frshflux_nodes], s=10, color=\"blue\", alpha=1, label='frshflux',zorder=20001)\n",
    "#======================================================================\n",
    "\n",
    "\n",
    "\n",
    "#QUI CONTROLLA CHE COSA SUCCEDE ALLE LOGNITUDINI E LATITUDINI============================================\n",
    "for edge, weight in zip(W.edges(), edge_weights):\n",
    "    normalized_weight = (weight - min(edge_weights)) / (max(edge_weights) - min(edge_weights))\n",
    "    edge_width = 1 + 4 * normalized_weight\n",
    "    #print(\"prima\",pos[edge[0]][1], pos[edge[1]][1])\n",
    "    lon1 = (np.mod(pos[edge[0]][1], 180))\n",
    "    lon2 = (pos[edge[1]][1])\n",
    "\n",
    "    \n",
    "    ax.plot([pos[edge[0]][0], pos[edge[1]][0]], [lon1, lon2], alpha=0.1, color=\"m\", linewidth=edge_width, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Plot nodes\n",
    "ax.scatter([pos[node][0] for node in eke_nodes], [pos[node][1] for node in eke_nodes], \n",
    "           s=10, color=\"green\", alpha=0.9, label='SST tropics', zorder=20001, transform=ccrs.PlateCarree())\n",
    "ax.scatter([pos[node][0] for node in frshflux_nodes], [pos[node][1] for node in frshflux_nodes],\n",
    "           s=10, color=\"purple\", alpha=1, label='SST world', zorder=20001, transform=ccrs.PlateCarree())\n",
    "#======================================================================\n",
    "\n",
    "\n",
    "font = {\"fontname\": \"Helvetica\", \"color\": \"k\",  \"fontsize\": 14}\n",
    "ax.set_title(\"Network Map \\n SST (tropics) vs SST (world)\", font)\n",
    "text1 = AnchoredText(\"monthly data 2013-2016, res=5deg\",\n",
    "                    loc=3, prop={'size': 8.5}, frameon=True, zorder=24000)\n",
    "\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "# Change font color for legend\n",
    "#font[\"color\"] = \"r\"\n",
    "\n",
    "\n",
    "# Resize figure for label readability\n",
    "ax.margins(0.1, 0.05)\n",
    "#fig.savefig()\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f40b79d-d056-464f-b40d-f60569b9dd49",
   "metadata": {},
   "source": [
    "<font size=8> Interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b265e-a23b-45f9-bfea-10806de6007a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cdadc2-7619-4519-b6ec-cc29606ed8dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "pos = {node: (np.mod(node[1], 360), node[0]) for node in W.nodes()}  # No need to modify longitudes, assuming they are already in the range -180 to +180\n",
    "\n",
    "# Extract edge weights for edge width\n",
    "edge_weights = [data['weight'] for _, _, data in W.edges(data=True)]\n",
    "\n",
    "# Separate nodes based on type\n",
    "eke_nodes = [node for node in W.nodes() if node[2] == 'eke']\n",
    "frshflux_nodes = [node for node in W.nodes() if node[2] == 'frsh_flux']\n",
    "\n",
    "# Create a Plotly graph object\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot 'eke' nodes\n",
    "fig.add_trace(go.Scattergeo(\n",
    "    lon=[pos[node][0] for node in eke_nodes],\n",
    "    lat=[pos[node][1] for node in eke_nodes],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=7,\n",
    "        color=\"green\",\n",
    "    ),\n",
    "    name='SST tropics',\n",
    "))\n",
    "\n",
    "# Plot 'frshflux' nodes\n",
    "fig.add_trace(go.Scattergeo(\n",
    "    lon=[pos[node][0] for node in frshflux_nodes],\n",
    "    lat=[pos[node][1] for node in frshflux_nodes],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=7,\n",
    "        color=\"purple\",\n",
    "    ),\n",
    "    name='SST world',\n",
    "))\n",
    "\n",
    "# Plot edges\n",
    "for edge, weight in zip(W.edges(), edge_weights):\n",
    "    normalized_weight = (weight - min(edge_weights)) / (max(edge_weights) - min(edge_weights))\n",
    "    edge_width = 1 + 4 * normalized_weight\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        lon=[pos[edge[0]][0], pos[edge[1]][0], None],  # None to create a gap between segments\n",
    "        lat=[pos[edge[0]][1], pos[edge[1]][1], None],\n",
    "        mode='lines',\n",
    "        line=dict(\n",
    "            width=edge_width,\n",
    "            color='rgba(128, 0, 128, 0.1)',\n",
    "        ),\n",
    "        text=f'Correlation: {weight}',\n",
    "        hoverinfo='text',\n",
    "    ))\n",
    "\n",
    "# Update layout for better visibility\n",
    "fig.update_layout(\n",
    "    geo=dict(\n",
    "        showland=True,\n",
    "        landcolor=\"#D3D3D3\",  # Set the color for the land\n",
    "        projection_type=\"orthographic\"\n",
    "    ),\n",
    "    width=1000,  # Set the width of the plot\n",
    "    height=800   # Set the height of the plot\n",
    ")\n",
    "\n",
    "# Set legend entries\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        traceorder='normal',\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Save the plot as an HTML file\n",
    "#fig.write_html(\"SSTtropSSTwrld_2013_16_step20.html\")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082dabb-789c-4758-a109-ff7e8944a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "(4.5,180.5)(4.5,175.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1d4827-93fe-43d1-b57e-17ff448d4e9a",
   "metadata": {},
   "source": [
    "# node degree map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29056201-6af0-4dd1-8b4f-94258eee6305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate layout for visualization with Cartopy\n",
    "pos = {node: (node[1], node[0]) for node in W.nodes()}  # Convert longitudes to the range -180 to +180\n",
    "\n",
    "# Calculate node degrees\n",
    "degrees = dict(W.degree())\n",
    "\n",
    "# Extract node positions, sizes, and colors based on node type\n",
    "node_positions = np.array([pos[node] for node in W.nodes()])\n",
    "node_sizes = np.array([degrees[node] for node in W.nodes()])\n",
    "node_colors = ['green' if node[2] == 'eke' else 'purple' for node in W.nodes()]\n",
    "\n",
    "# Create a scatter plot with node sizes proportional to degree and colors based on type\n",
    "fig, ax = plt.subplots(figsize=(12, 12), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.add_feature(cfeature.LAND, zorder=1, edgecolor='k')\n",
    "\n",
    "# Scatter plot nodes with a small black dot at the center\n",
    "sc = ax.scatter(\n",
    "    node_positions[:, 0], node_positions[:, 1], s=node_sizes * 5,\n",
    "    c=node_colors, alpha=0.6, edgecolors='k', linewidths=0.5\n",
    ")\n",
    "\n",
    "# Add a black dot at the center of each circle representing a node\n",
    "#ax.scatter(node_positions[:, 0], node_positions[:, 1], color='black', s=2)\n",
    "\n",
    "# Set the geographical extent to cover the entire world\n",
    "ax.set_extent([-180, 180, -90, 90], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add world map and gridlines\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
    "ax.gridlines(linewidth=0.5, linestyle='dashed', alpha=0, draw_labels=True)\n",
    "\n",
    "# Title\n",
    "font = {\"fontname\": \"Helvetica\", \"color\": \"k\",  \"fontsize\": 14}\n",
    "ax.set_title(\"Node Degree Map \\n SST (tropics) vs PREC (world) in Jan 2013 - Oct 2013\", font)\n",
    "text1 = AnchoredText(\"weekly data 2013-2013, res=5deg\",\n",
    "                    loc=3, prop={'size': 8.5}, frameon=True, zorder=24000)\n",
    "ax.add_artist(text1)\n",
    "\n",
    "\n",
    "# Resize figure for label readability\n",
    "ax.margins(0.1, 0.05)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"SSTtropSSTwrld_2013_13_step20.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c0f73-78e2-4e56-9431-26453a2e0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "20*0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb5574a-7d95-4817-a23e-110638b7b747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88da1730-9a6c-4566-8793-805698da01e0",
   "metadata": {},
   "source": [
    "# BETWENNESS CENTRALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64cdb8f-2821-4852-91d2-ab054e380752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate betweenness centrality for each node\n",
    "betweenness_centrality = nx.betweenness_centrality(W)\n",
    "\n",
    "# Generate layout for visualization with Cartopy\n",
    "pos = {node: (node[1], node[0]) for node in W.nodes()}  # Convert longitudes to the range -180 to +180\n",
    "\n",
    "# Extract node positions, sizes, and colors based on betweenness centrality\n",
    "node_positions = np.array([pos[node] for node in W.nodes()])\n",
    "node_sizes = np.array([betweenness_centrality[node] for node in W.nodes()])\n",
    "node_colors = np.array([betweenness_centrality[node] for node in W.nodes()])\n",
    "\n",
    "# Create a scatter plot with node sizes proportional to betweenness centrality\n",
    "fig, ax = plt.subplots(figsize=(12, 12), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.add_feature(cfeature.LAND, zorder=1, edgecolor='k')\n",
    "\n",
    "# Scatter plot nodes with a small black dot at the center\n",
    "sc = ax.scatter(\n",
    "    node_positions[:, 0], node_positions[:, 1], s=node_sizes * 30,\n",
    "    c=node_colors, cmap='viridis', edgecolors='k', linewidths=0.5\n",
    ")\n",
    "\n",
    "\n",
    "# Set the geographical extent to cover the entire world\n",
    "ax.set_extent([-180, 180, -90, 90], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add world map and gridlines\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
    "ax.gridlines(linewidth=0.5, linestyle='dashed', alpha=0, draw_labels=True)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(sc, orientation='horizontal', pad=0.04)\n",
    "cbar.set_label('Betweenness Centrality')\n",
    "\n",
    "# Title\n",
    "font = {\"fontname\": \"Helvetica\", \"color\": \"k\", \"fontsize\": 14}\n",
    "ax.set_title(\"Betweenness Centrality Map\", font)\n",
    "\n",
    "# Resize figure for label readability\n",
    "ax.margins(0.1, 0.05)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb751b-9cda-4c9b-ac71-3452b980152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "posss = nx.spring_layout(W)  # Choose a layout for the nodes\n",
    "nx.draw(W, posss, with_labels=False, node_size=30, node_color=\"skyblue\", font_size=10, font_color=\"black\", font_weight=\"bold\", edge_color=\"gray\", linewidths=1, alpha=0.7)\n",
    "\n",
    "# Show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc3ad0-3066-4164-810a-fa50130c1cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c09b304c-0f52-44a4-84a7-7b31f3596413",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94a8aaa-d774-41a8-8fab-146485a9c56a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If i wanted only one more step of parallelisation, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80a08d-1f76-4ae6-af7a-04ba083050d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MORE PARALLELISING\n",
    "def inner_parallel_func(lat_lon):\n",
    "    lat, lon = lat_lon\n",
    "    S = nx.Graph()\n",
    "    if not np.any(np.isnan(eke_downsized[:, lat, lon])):\n",
    "        # Extract time series at the current 'eke' position\n",
    "        time_series_eke = eke_downsized[:, lat, lon]\n",
    "\n",
    "        # Iterate over all positions (lat, lon) for 'frshflux'\n",
    "        for lat_frshflux in range(num_latitudes):\n",
    "            for lon_frshflux in range(num_longitudes):\n",
    "                if not np.any(np.isnan(frshflux_downsized[:, lat_frshflux, lon_frshflux])):\n",
    "                    # Extract time series at the current 'frshflux' position\n",
    "                    time_series_frshflux = frshflux_downsized[:, lat_frshflux, lon_frshflux]\n",
    "\n",
    "                    # Check for NaN values\n",
    "                    if not (np.any(np.isnan(time_series_eke)) or np.any(np.isnan(time_series_frshflux))):\n",
    "                        # Calculate Pearson correlation coefficient\n",
    "                        correlation, _ = pearsonr(time_series_eke, time_series_frshflux)\n",
    "\n",
    "                        # Check if correlation is above the threshold\n",
    "                        if abs(correlation) > correlation_threshold:\n",
    "                            # Add nodes to the graph\n",
    "                            node_eke = (ds.lat.values[::step][lat], ds.lon.values[::step][lon], 'eke')\n",
    "                            node_frshflux = (ds.lat.values[::step][lat_frshflux], ds.lon.values[::step][lon_frshflux], 'frshflux')\n",
    "                            S.add_node(node_eke)\n",
    "                            S.add_node(node_frshflux)\n",
    "\n",
    "                            # Add weighted edge with correlation coefficient as weight\n",
    "                            S.add_weighted_edges_from([(node_eke, node_frshflux, correlation)])\n",
    "    return S\n",
    "\n",
    "# Modify parallel_func to use inner_parallel_func\n",
    "def parallel_func(lat):\n",
    "    with ProcessPoolExecutor(max_workers=n_cpu) as executor:\n",
    "        S_list = list(executor.map(inner_parallel_func, [(lat, lon) for lon in range(num_longitudes)]))\n",
    "    return S_list\n",
    "\n",
    "# Measure the total computation time\n",
    "start_time_total = timeit.default_timer()\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=n_cpu) as executor:\n",
    "    # Use executor.map to parallelize parallel_func\n",
    "    results_list = list(executor.map(parallel_func, range(num_latitudes)))\n",
    "\n",
    "end_time_total = timeit.default_timer()\n",
    "\n",
    "# Print the total computation time\n",
    "print(f\"Total computation time for the entire script: {end_time_total - start_time_total:.2f} seconds\")\n",
    "\n",
    "# Access individual results\n",
    "final_S_list = [result for sublist in results_list for result in sublist]\n",
    "\n",
    "# Continue with the rest of your script...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f06a8-6a3e-418a-b837-25c63cbb62aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd745b-18ea-47a3-b1e7-adf3aeafe829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1 Python 3 (based on the module python3/2023.01)",
   "language": "python",
   "name": "python3_2023_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
